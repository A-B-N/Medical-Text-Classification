# Medical-Text-Classification
#1. Initial Processes: The libraries required for the project were imported first. The train and the test data were opened and read line by line. The training sample had 14438 samples and the test had 14442 samples. The sentences were then tokenized and the class label and the training data were separated and stored in a separate file named doc for the purpose of analysis. Using Counter function the number of documents in each class was then found out. Class 1 had 3163 documents, Class 2 had 1494, Class 3 had 1925 documents, Class 4 had 3051 and class 5 had 4805 documents, thus showing that the documents are not equally distributed among the 5 classes. Then the test data was appended into the doc for the purpose of analysis.

#2. Preprocessing of the data: After appending the test data into the doc, the doc had been preprocessed in order to make it appropriate for the further analysis. A set of five functions performing different preprocessing operations had been used. The library ‘nltk’ imported in the previous step was used majorly for this part. The first function used was stopWord_remover, that was used to remove the stop words by using the English.txt file in the ‘nltk’ library. After removing the stop words the document was passed into the second function, punctuation_remover, that removed the punctuations present in the sentences. After removing the punctuations, the document was passed into the filterlen function, that removes all the words in the document that are of length less that the minimum length passed as the parameter. All the words of length less than 4 were filtered out in this project.
After filtering out the smaller words, the document was passed in to the lemmatize function to lemmatize the document. ‘WordNetLemmatizer’ available in the ‘nltk’ library was used for the purpose of lemmatizing the documents. The words were lemmatized by taking their verbal form as the base. After lemmatization, the document was passed into the to_lowercase_ and_filtering function, that converts all the words in the document into lower case and then filtered out all the characters, particularly the digits present in the document. After the completion of all these processes, the document was prepared to be converted in to a csr matrix.

#3. Building up of the Sparse Matrix: The function build_matrix is used to transform the document at the end of the previous step which is represented as a list of lists of words into a sparse matrix, by using the count of the number of occurrences of each word present in the document. The function csr_info is being used to display information about the size of the matrix-no of
rows, no of columns and no of non-zero entries. The matrix formed was the scaled using Inverse Document Frequency in the csr_IDF function. The scared matrix is then normalized using csr_l2normalize which uses L2-norm to normalize the matrix. This normalized matrix had 28880 rows and 55021 columns. This matrix can then be used for further processing.

#4. Finding the Nearest Neighbor: The normalized sparse matrix is then split into training and test data. A loop had been created for the length of the test matrix and the cosine similarity was computed as a dot product of every ith row of the test with the entire training matrix in the cosine_similarity function. This value of similarity is then zipped and passed into the NN function which computes the neighbors for a given value of k (default value of which had been taken as 3) and a similarity of 0.5. After computing the neighbors, the ‘sim’ list was sorted. This sorted sim list was then taken to classify the test documents into different classes using the ‘majority votes’ approach. The result of the sum was used as the tie-breaker, in case there was a tie in the majority votes. After this the votes were sorted and passed as class labels. The labels are then read in the prediction list, which calls the NN function. The class labels were then written in to the file named,”Predictions.dat”.

#5. Execution: For the first submission, a k value of 5 was used. F1- Score of 0.7433 was obtained for this submission. For the next submission, a k value of 7 was used. F1- score of 0.7520 was obtained for this submission. For the subsequent submissions, k values of 10, 15,20,25 and obtained the corresponding F-1 scores of 0.7594, 0.7638, 0.7690 and 0.7690 .
