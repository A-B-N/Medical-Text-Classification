{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Medical Text Classification using k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ajith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ajith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essential libraries needed for the program\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict,Counter\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "with open(\"train.dat\",'r') as fh:\n",
    "    lines= fh.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data\n",
    "with open(\"test.dat\",'r') as fh:\n",
    "    lines1= fh.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaks down sentences into words\n",
    "tr_data=[l.split() for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data=[l.split() for l in lines1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends train data into a separate doc for processing and separates the label and data\n",
    "Class=[]\n",
    "docs=[]\n",
    "for i in range(0,len(tr_data)):\n",
    "    Class.append(tr_data[i][0])\n",
    "    docs.append(tr_data[i][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'5': 4805, '1': 3163, '4': 3051, '3': 1925, '2': 1494})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_frame = pd.DataFrame()\n",
    "data_frame['text']= docs[:]\n",
    "data_frame['class']= Class[:]\n",
    "Y= Counter(data_frame['class'])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28880\n"
     ]
    }
   ],
   "source": [
    "# appends the test data into the doc created for the purpose of analysis\n",
    "for i in range(0,len(te_data)):\n",
    "    docs.append(te_data[i])\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing steps:\n",
    "# the function to remove the stopwords in the documents \n",
    "def stopWords_remover(docs):\n",
    "    doc1=[]\n",
    "    stopWords=set(stopwords.words('english'))\n",
    "    for doc in docs:\n",
    "        word1=[]\n",
    "        for word in doc:\n",
    "            if word not in stopWords:\n",
    "                word1.append(word)\n",
    "        doc1.append(word1)\n",
    "    return doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function which removes the punctuations used in the sentences using Regular Expressions(re) library\n",
    "def punctuation_remover(docs):\n",
    "    doc1=[]\n",
    "    for doc in docs:\n",
    "        word1=[]\n",
    "        for word in doc:\n",
    "            word=re.sub(r'[^\\w\\s]','',word)\n",
    "            if word != '':\n",
    "                word1.append(word)\n",
    "        doc1.append(word1)\n",
    "    return doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes the words that are below the minlength set\n",
    "def filterlen(docs,minlen):\n",
    "    return[[t for t in d if len(t)>=minlen]for d in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function lemmatizes the document using the WordNetLemmatizer() available in the nltk library\n",
    "def lemmatize(docs):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    doc1=[]\n",
    "    for doc in docs:\n",
    "        word1=[]\n",
    "        for word in doc:\n",
    "            lemma=lemmatizer.lemmatize(word,'v')\n",
    "            word1.append(lemma)\n",
    "        doc1.append(word1)\n",
    "    return doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function that changes all the words into lowercase and filters out any digits present in the document\n",
    "def tolowerCase_and_filtering(docs):\n",
    "    doc1=[]\n",
    "    for doc in docs:\n",
    "        word1=[]\n",
    "        for word in doc:\n",
    "            word1.append(word.lower())\n",
    "            for char in word:\n",
    "                if(not char.isalpha()):\n",
    "                    word1.remove(word.lower())\n",
    "                    break\n",
    "        doc1.append(word1)\n",
    "    return doc1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1= stopWords_remover(docs)\n",
    "\n",
    "doc2= punctuation_remover(doc1)\n",
    "\n",
    "doc3= filterlen(doc2,4)\n",
    "\n",
    "doc4= lemmatize(doc3)\n",
    "\n",
    "doc5= tolowerCase_and_filtering(doc4)         \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building up of the sparse matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    \n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  \n",
    "    n = 0  \n",
    "\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scales the matrix and normalizes its rows \n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  \n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  \n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 28880, ncols 55021, nnz 1916480]\n",
      "(28880, 55021)\n"
     ]
    }
   ],
   "source": [
    "doc6 = build_matrix(doc5)\n",
    "csr_info(doc6)\n",
    "doc7 = csr_idf(doc6, copy=True)\n",
    "doc8 = csr_l2normalize(doc7, copy=True)\n",
    "print(doc8.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting up of the train and test data for the purpose of analysis\n",
    "train=doc8[0:14438]\n",
    "test=doc8[14438:]\n",
    "train_Classes=Class[0:14438]\n",
    "test_Classes=Class[14438:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function that computes the cosine similarity between the test and train matrices\n",
    "import math\n",
    "def cosine_similarity(test,train):\n",
    "    dot_prod=test.dot(train.T)\n",
    "    sim=list(zip(dot_prod.indices,dot_prod.data))\n",
    "    return sim\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function that finds the nearest neighbour and classifies them using maximum vote method\n",
    "def NN(test,sim,trainlabel,k=3):\n",
    "    #sim=cosine_similarity(test,train)\n",
    "    if len(sim) ==0:\n",
    "        if np.random.rand()>0.5:\n",
    "            return '+'\n",
    "        else:\n",
    "            return '-'\n",
    "    sim.sort(key=lambda test:test[1], reverse= True)\n",
    "    c=Counter(trainlabel[i[0]] for i in sim[:k] ).most_common(2)\n",
    "    if len(c)<2 or c[0][1]>c[1][1]:\n",
    "        return c[0][0]\n",
    "    c=defaultdict(float)\n",
    "    for i in sim[:k]:\n",
    "         c[trainlabel[i[0]]] += i[1]\n",
    "    sortedVotes=sorted(c.items(),key=lambda x:x[1], reverse= True)[0][0]\n",
    "    return sortedVotes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding the prediction and copying them into a file\n",
    "sim=[]\n",
    "predictions=[]\n",
    "file=open(\"Predictions7.dat\",\"w+\")\n",
    "for i in test:\n",
    "    sim=cosine_similarity(i,train)\n",
    "    #print(sim)\n",
    "    predictions=NN(i,sim,train_Classes,25)\n",
    "    file.write(str(predictions) +\"\\n\" )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
